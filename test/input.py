import os
import time
import numpy as np
import sounddevice as sd
import soundfile as sf
# import wave
from scipy.io import wavfile
from openai import OpenAI
from dotenv import load_dotenv
from pathlib import Path

def record_smart(
    save_dir: Path,
    samplerate=16000,
    channels=1,
    dtype="int16",
    start_threshold=300,
    silence_threshold=500,
    silence_duration=0.7,
):
    """æ™ºæ…§éŒ„éŸ³"""
    import os
    from scipy.io import wavfile
    
    frames = []
    recording = False
    silence_start = None
    
    voice_input_path = os.path.join(str(save_dir), "VoiceChatRoomDemo_input.wav")
    print(f"éŒ„éŸ³æª”è·¯å¾‘: {voice_input_path}")

    print("è«‹é–‹å§‹èªªè©±...")

    with sd.InputStream(samplerate=samplerate, channels=channels, dtype=dtype) as stream:
        while True:
            data, _ = stream.read(int(samplerate * 0.1))
            volume = np.abs(data).mean()

            if not recording and volume > start_threshold:
                recording = True
                frames.append(data)
                print("ğŸ”´ éŒ„éŸ³ä¸­...")
                continue

            if recording:
                frames.append(data)

                if volume < silence_threshold:
                    if silence_start is None:
                        silence_start = time.time()
                    elif time.time() - silence_start > silence_duration:
                        print("éŒ„éŸ³çµæŸï¼")
                        break
                else:
                    silence_start = None

    audio_data = np.concatenate(frames, axis=0)

    # âœ… ç”¨ scipy å¯«å…¥ï¼Œä¸ç”¨ wave æ¨¡çµ„
    wavfile.write(voice_input_path, samplerate, audio_data)

    return Path(voice_input_path)

def speech_to_text(client: OpenAI, audio_path: Path):
    """ä½¿ç”¨GPTæ¨¡å‹è¾¨è­˜èªéŸ³å…§å®¹ï¼Œä¸¦è½‰æˆæ–‡å­—(speech-to-text)"""
    transcription = client.audio.transcriptions.create(
        # transcribeæ¨¡å‹ï¼Œæä¾›speech-to-textåŠŸèƒ½
        model="gpt-4o-mini-transcribe",
        # è¼‰å…¥èªéŸ³æª”æ¡ˆ
        file=audio_path.open("rb")
    )
    text = transcription.text.strip()
    return text


def gpt_response(client: OpenAI, model: str, user_input: str):
    """GPTæ¨¡å‹å›è¦†"""
    response = client.responses.create(
        model=model,
        input=user_input
    )
    reply_text = response.output_text
    return reply_text


def text_to_speech(client: OpenAI, text: str, save_dir: Path):
    """GPTæ¨¡å‹å°‡æ–‡å­—å›è¦†è½‰æˆèªéŸ³å›è¦†"""
    # èªéŸ³è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
    output_path = save_dir/"VoiceChatRoomDemo_output.mp3"
    # ttsæ¨¡å‹ï¼Œæä¾›text-to-speechåŠŸèƒ½
    with client.audio.speech.with_streaming_response.create(
        model="gpt-4o-mini-tts",
        # è²éŸ³ç¨®é¡ï¼Œåƒçœ‹ https://www.openai.fm/
        voice="alloy",
        input=text
    ) as speech:
        speech.stream_to_file(output_path)

    # å°‡ mp3 è§£ç¢¼ç‚ºéŸ³è¨Šè³‡æ–™
    data, samplerate = sf.read(output_path, dtype="float32")
    sd.play(data, samplerate)
    # ç­‰å¾…æ’­æ”¾çµæŸ
    sd.wait()


def api_key():
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("API keyä¸å­˜åœ¨ï¼Œè«‹æª¢æŸ¥.envæª”æ¡ˆ")

    return api_key


def main():

            # å¼·åˆ¶åˆ‡æ›å·¥ä½œç›®éŒ„
    import os
    os.chdir("C:/Users/selina/Documents/candy_store_project")

    # èªéŸ³å­˜æª”ç›®éŒ„
    save_dir = Path("C:/Users/selina/Documents/candy_store_project/saved")

    # å°å‡ºç¢ºèª
    print(f"å·¥ä½œç›®éŒ„: {os.getcwd()}")
    print(f"save_dir: {save_dir}")
    print(f"save_dir å­˜åœ¨: {save_dir.exists()}")



    client = OpenAI(api_key=api_key())
    model = "gpt-4.1-nano"
    # èªéŸ³å­˜æª”ç›®éŒ„
    current_dir = Path(__file__).resolve().parent
    save_dir = current_dir/"saved"
    save_dir.mkdir(parents=True, exist_ok=True)
    print("æ­¡è¿é€²å…¥èªéŸ³èŠå¤©å®¤ï¼ˆèªªã€ŒçµæŸå°è©±ã€æˆ–ã€Œé›¢é–‹èŠå¤©å®¤ã€å¯çµæŸï¼‰")

    # å„²å­˜èŠå¤©ä¸Šä¸‹æ–‡
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å‹å–„çš„èªéŸ³åŠ©ç†ï¼Œè«‹ç°¡æ½”å›ç­”ä½¿ç”¨è€…çš„å•é¡Œ"}
    ]

    while True:
        print("è«‹é–‹å§‹èªªè©±...")
        voice_input_path = record_smart(save_dir)
        user_text = speech_to_text(client, voice_input_path)
        print(f"ä½ èªª: {user_text}")

        if any(kw in user_text for kw in ["çµæŸå°è©±", "é›¢é–‹èŠå¤©å®¤"]):
            print("å·²çµæŸèªéŸ³èŠå¤©å®¤ï¼")
            text_to_speech(client, "å¥½çš„ï¼ŒæœŸå¾…ä¸‹æ¬¡å†èŠã€‚", save_dir)
            break

        messages.append({"role": "user", "content": user_text})
        reply_text = gpt_response(client, model, messages)
        print(f"GPT: {reply_text}")
        text_to_speech(client, reply_text, save_dir)
        messages.append({"role": "assistant", "content": reply_text})




if __name__ == "__main__":
    print("=" * 30)
    main()
    print("=" * 30)
