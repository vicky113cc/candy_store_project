# -*- coding: utf-8 -*-
"""20251105Train_YOLOv11_Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13YWYees2m33K6GXLuWB1M_dOuTnmAvRW

# 在 Google Colab 中訓練 YOLO 模型

**作者：** Evan Juras，[EJ Technology Consultants](https://ejtech.io)

**上次更新：** 2025 年 1 月 3 日

**GitHub：** [訓練與部署 YOLO 模型](https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models)

# 簡介

本筆記本使用 [Ultralytics](https://docs.ultralytics.com/) 和自訂資料集來訓練 YOLO11、YOLOv8 或 YOLOv5 目標偵測模型。完成本 Colab 後，您將獲得一個自訂 YOLO 模型，可在您的 PC、手機或邊緣裝置（例如 Raspberry Pi）上運行。

<p align=center>

<img src="https://s3.us-west-1.amazonaws.com/evanjuras.com/img/yolo-model-demo.gif" height="360"><br>

<i>YOLO 自訂糖果偵測模型示範！ </i>

</p>

我製作了一個 YouTube 視頻，逐步講解本指南。我建議您在學習本筆記本的同時觀看影片。

<p align=center>

<a href="https://youtu.be/r0RspiLG260" target="_blank"><img src="https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/Train_YOLO_Thumbnaild.

<i>點此觀看影片！ </i></a>

</p>

**重要提示：本筆記本將持續更新，以確保其與新版本的 Ultralytics 和 YOLO 相容。如果您發現 YouTube 影片與本筆記本有任何差異，請務必以本筆記本為準！ **

### 在 Colab 中工作

Colab 在您的瀏覽器中提供了一個完整的虛擬機，其中包含 Linux 作業系統、檔案系統、Python 環境，最重要的是，還配備了免費的 GPU。我們將在此環境中安裝 PyTorch 和 Ultralytics，並使用它來訓練我們的模型。只需點擊此筆記本中代碼段的「播放」按鈕，即可在虛擬機器上執行它們。

### 導航

若要瀏覽此筆記本，請使用左側邊欄中的目錄在各章節之間跳轉。

**驗證 NVIDIA GPU 可用性**

請確保您使用的是配備 GPU 的電腦。方法是：點擊頂部功能表列中的“運行時”->“更改運行時類型”，然後在“硬體加速器”部分選擇 GPU 選項。點擊以下程式碼區塊中的「執行」按鈕，驗證 NVIDIA GPU 是否已連線並準備就緒，可用於訓練。
"""

!nvidia-smi

"""#1.&nbsp;Gather and Label Training Images

在開始訓練之前，我們需要收集並標註用於訓練目標偵測模型的影像。對於概念驗證模型來說，200 張圖像是一個不錯的起點。訓練影像中應該包含隨機物體以及目標物體，並且背景和光照條件應多樣化。

收集影像有兩種方法：

* 自行拍攝目標物件並標註，建立自訂資料集（通常這種方法能獲得最佳效能）

* 從 Roboflow Universe、Kaggle 或 Google Images V7 等來源取得預製資料集

如果您想建立自己的資料集，可以使用多種影像標註工具。 Label Studio 是不錯的選擇，它是一款免費開源的標註工具，工作流程簡單，同時也提供更進階的功能。我製作了一個 YouTube 影片（連結稍後添加），詳細講解如何使用 Label Studio 為圖像添加標籤。

<p align=center>

<img src="https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/label-studio-example.PNG" height="380"><br>

<i>使用 Label Studio 標註的糖果圖片範例。 </i>

</p>

如果您使用 Label Studio 標註並匯出圖像，它們會匯出到名為 `project.zip` 的檔案中，該檔案包含以下內容：

- 一個包含圖像的 `images` 資料夾

- 一個包含 YOLO 標註格式標籤的 `labels` 資料夾

- 一個包含所有類別的 `classes.txt` 標籤映射文件

- 一個包含 Label Studio 特定資訊的 `notes.json` 檔案（此檔案可以忽略）

如果您從其他來源（例如 Roboflow Universe）取得資料集，或使用其他工具標註資料集，請確保檔案組織在相同的資料夾結構中。

<p align=center>

<img src="https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/zipped-data-example.png" height=""><br>

<i>請按照此處所示的資料夾結構整理您的資料。您可以參考我的<a href="https://s3.us-west-1.amazonaws.com/evanjuras.com/resources/candy_data_06JAN25.zip">糖果檢測資料集</a>作為範例。 </i>

</p>

資料集建置完成後，將其放入上述文件結構中，並壓縮成 `data.zip` 文件，即可進行下一步。

# 2.&nbsp;Upload Image Dataset and Prepare Training Data

接下來，我們將上傳資料集並準備使用 YOLO 進行訓練。我們將資料集拆分為訓練集和驗證集資料夾，並自動產生用於訓練模型的設定檔。

## 2.1 上傳圖片

首先，我們需要將資料集上傳到 Colab。以下是將 `data.zip` 資料夾移至此 Colab 實例的幾種方法。

**選項 1：透過 Google Colab 上傳**

點擊瀏覽器左側的「檔案」圖標，然後點擊「上傳到會話儲存」圖標，將 `data.zip` 檔案上傳到 Google Colab 實例。選擇要上傳的 zip 資料夾。

<p>

<br>

<img src="https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/upload-colab-files.png" height="240">

</p>

**選項 2：從 Google 雲端硬碟複製**

您也可以將圖片上傳到您的個人 Google 雲端硬碟，將該雲端硬碟掛載到此 Colab 工作階段中，然後將圖片複製到 Colab 檔案系統。如果您希望提前上傳圖片，避免每次重新啟動 Colab 時都等待圖片上傳，此選項非常適用。如果您有超過 50MB 的圖片，我建議您使用此選項。

首先，將 `data.zip` 檔案上傳到您的 Google 雲端硬碟，並記下上傳到的資料夾。將 `MyDrive/path/to/data.zip` 替換為您的 zip 檔案路徑。 （例如，我將 zip 檔案上傳到了名為「candy-dataset1」的資料夾，因此路徑應為 `MyDrive/candy-dataset1/data.zip`）。然後，執行以下程式碼區塊，將您的 Google 雲端硬碟掛載到此 Colab 會話並將資料夾複製到此檔案系統中。
"""

from google.colab import drive
drive.mount('/content/gdrive')

!cp /content/gdrive/MyDrive/path/to/data.zip /content

"""**選項 3：使用我的糖果檢測或硬幣檢測資料集**

如果您只想在預先準備好的資料集上測試流程，可以使用我的以下資料集之一：

* [糖果圖像資料集](https://s3.us-west-1.amazonaws.com/evanjuras.com/resources/candy_data_14DEC24.zip)，其中包含 162 張常見糖果（例如彩虹糖、士力架等）的圖片。

* [硬幣圖像資料集](https://s3.us-west-1.amazonaws.com/evanjuras.com/resources/YOLO_coin_data_12DEC30.zip)，其中包含 750 張美國硬幣（例如 1 美分、10 美分、5 美分和 25 美分）的圖片。

執行以下程式碼區塊即可下載其中一個資料集。在接下來的筆記本中，我將以糖果偵測資料集為例進行說明。
"""

# To use my one of pre-made dataset instead of your own custom dataset, download it here (control which dataset is downloaded by commenting out either line)
!wget -O /content/data.zip https://s3.us-west-1.amazonaws.com/evanjuras.com/resources/candy_data_06JAN25.zip # Candy dataset
#!wget -O /content/data.zip https://s3.us-west-1.amazonaws.com/evanjuras.com/resources/YOLO_coin_data_12DEC30.zip # Coin dataset

"""## 2.2 Split images into train and validation folders

此時，無論您使用選項 1、2 還是 3，都應該可以點擊左側的資料夾圖標，並在檔案清單中看到 `data.zip` 檔案。接下來，我們將解壓縮 `data.zip` 文件，並建立一些資料夾來存放圖片。運行以下程式碼區塊即可解壓縮資料。
"""

# Unzip images to a custom data folder
!unzip -q /content/data.zip -d /content/custom_data

"""Ultralytics 需要特定的資料夾結構來儲存模型的訓練資料。根資料夾名為“data”。其下有兩個主要資料夾：

* **Train**：這些是用於訓練模型的實際圖像。在一個訓練週期中，訓練集中的每張影像都會輸入到神經網路中。訓練演算法會調整網路權重以適應影像中的資料。

* **Validation**：這些圖像用於在每個訓練週期結束時檢查模型的性能。

每個資料夾下方都有一個「images」資料夾和一個「labels」資料夾，分別用於存放影像檔案和標註檔案。

I wrote a Python script that will automatically create the required folder structure and randomly move 90% of dataset to the "train" folder and 10% to the "validation" folder. Run the following code block to download and execute the scrpt.
"""

!wget -O /content/train_val_split.py https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/utils/train_val_split.py

# TO DO: Improve robustness of train_val_split.py script so it can handle nested data folders, etc
!python train_val_split.py --datapath="/content/custom_data" --train_pct=0.9

"""# 3. 安裝要求 (Ultralytics)

接下來，我們將在此 Google Colab 實例中安裝 Ultralytics 庫。這個 Python 函式庫將用於訓練 YOLO 模型。
"""

!pip install ultralytics

"""# 4.&nbsp;Configure Training

在開始訓練之前，還有最後一步：我們需要建立 Ultralytics 訓練配置 YAML 檔案。該文件指定了訓練資料和驗證資料的位置，並定義了模型的類別。範例設定檔模型可在此處取得：[https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco128.yaml](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/ultralytics/ultralytics/blob/main/ultralytics/cfgmlsets/coco)。

執行以下程式碼區塊即可自動產生 `data.yaml` 設定檔。請確保 `custom_data/classes.txt` 處存在標籤映射檔。如果您使用了 Label Studio 或我預先建立的資料集，則該檔案應該已經存在。如果您使用其他方式建立了資料集，則可能需要手動建立 `classes.txt` 檔案（有關其格式範例，請參閱[此處](https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/blob/main/doc/classes.txt)）。
"""

# Python function to automatically create data.yaml config file
# 1. Reads "classes.txt" file to get list of class names
# 2. Creates data dictionary with correct paths to folders, number of classes, and names of classes
# 3. Writes data in YAML format to data.yaml

import yaml
import os

def create_data_yaml(path_to_classes_txt, path_to_data_yaml):

  # Read class.txt to get class names
  if not os.path.exists(path_to_classes_txt):
    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')
    return
  with open(path_to_classes_txt, 'r') as f:
    classes = []
    for line in f.readlines():
      if len(line.strip()) == 0: continue
      classes.append(line.strip())
  number_of_classes = len(classes)

  # Create data dictionary
  data = {
      'path': '/content/data',
      'train': 'train/images',
      'val': 'validation/images',
      'nc': number_of_classes,
      'names': classes
  }

  # Write data to YAML file
  with open(path_to_data_yaml, 'w') as f:
    yaml.dump(data, f, sort_keys=False)
  print(f'Created config file at {path_to_data_yaml}')

  return

# Define path to classes.txt and run function
path_to_classes_txt = '/content/custom_data/classes.txt'
path_to_data_yaml = '/content/data.yaml'

create_data_yaml(path_to_classes_txt, path_to_data_yaml)

print('\nFile contents:\n')
!cat /content/data.yaml

"""# 5.&nbsp;Train Model

## 5.1 訓練參數

現在資料已整理完畢，設定檔也已創建，我們可以開始訓練了！首先，我們需要確定一些重要的參數。請造訪我的文章[本地訓練 YOLO 模型](https://www.ejtech.io/learn/train-yolo-models)以了解更多關於這些參數以及如何選擇它們的資訊。

**模型架構與大小（`model`）：**

YOLO11 模型有多種大小可供選擇，包括 `yolo11n.pt`、`yolo11s.pt`、`yolo11m.pt`、`yolo11l.pt` 和 `yolo11xl.pt`。較大的模型運行速度較慢，但準確率較高；而較小的模型運行速度較快，但準確率較低。我製作了一個簡短的 YouTube 視頻，對比了不同 YOLO 模型在 Raspberry Pi 5 和配備 RTX 4050 GPU 的筆記本電腦上的性能，[點擊此處觀看，了解它們的速度和精度](https://youtu.be/_WKS4E9SmkA)。如果您不確定要使用哪個模型大小，`yolo11s.pt` 是一個不錯的起點。

您也可以透過將 `yolov8` 或 `yolov5` 替換為 `yolo11` 來訓練 YOLOv8 或 YOLOv5 模型。

**訓練輪數（`epochs`）**

在機器學習中，「epoch」指的是對整個訓練資料集進行一次完整的遍歷。設定 epoch 數決定了模型的訓練時間。最佳 epoch 數取決於資料集的大小和模型架構。如果您的資料集包含少於 200 張影像，建議從 60 個訓練輪次開始。如果您的資料集包含超過 200 張圖像，建議從 40 個訓練輪次開始。

**解析度 (`imgsz`)**

解析度對模型的速度和準確率影響很大：解析度越低，模型速度越快，但準確率越低。 YOLO 模型通常在 640x640 解析度下進行訓練和推理。但是，如果您希望模型運行速度更快，或者知道您將處理低解析度影像，請嘗試使用更低的分辨率，例如 480x480。

## 5.2 Run Training!

執行以下程式碼區塊開始訓練。如果要使用不同的模型、訓練輪數或分辨率，請更改 `model`、`epochs` 或 `imgsz`。
"""

!yolo detect train data=/content/data.yaml model=yolo11s.pt epochs=60 imgsz=640

"""訓練演算法會解析訓練目錄和驗證目錄中的圖像，然後開始訓練模型。在每個訓練週期結束時，程式會在驗證資料集上執行模型，並報告所得的 mAP、精確率和召回率。隨著訓練的進行，mAP 通常應該隨著每個週期的訓練而增加。訓練會在達到 `epochs` 指定的週期數後結束。

> **注意：** 請確保訓練運行完成，因為優化器會在訓練結束時運行，從模型中移除不需要的層。

最佳訓練模型權重將保存在 `content/runs/detect/train/weights/best.pt` 檔案中。有關訓練的其他資訊保存在 `content/runs/detect/train` 資料夾中，其中包括一個 `results.png` 文件，該文件顯示了每個週期中損失、精確率、召回率和 mAP 的變化。

#6.&nbsp;Test Model

模型已訓練完成；現在是時候測試它了！以下命令將使用驗證資料夾中的映像運行模型，然後顯示前 10 張圖像的結果。這是確認模型是否如預期運作的好方法。點擊下方程式碼區塊中的「播放」按鈕，即可查看模型的效能。
"""

!yolo detect predict model=runs/detect/train/weights/best.pt source=data/validation/images save=True

import glob
from IPython.display import Image, display
for image_path in glob.glob(f'/content/runs/detect/predict/*.jpg')[:10]:
  display(Image(filename=image_path, height=400))
  print('\n')

"""模型應該在每張圖像中圍繞每個感興趣的物件繪製一個框。如果模型偵測物件的效果不佳，請嘗試以下幾點：

1. 仔細檢查資料集，確保沒有標註錯誤或衝突的樣本。

2. 增加訓練輪數。

3. 使用較大的模型尺寸（例如 `yolo11l.pt`）。

4. 在訓練資料集中新增更多影像。請觀看我的[資料集影片](https://www.youtube.com/watch?v=v0ssiOY6cfg)，了解如何擷取高品質的訓練影像並提高準確率。

您也可以將視訊檔案或其他圖像上傳到此筆記本，然後使用上述 `!yolo detect predict` 命令運行模型，其中 `source` 指向視訊檔案、圖像或圖像資料夾的位置。結果將保存在 `runs/detect/predict` 目錄中。

在影像上繪製方框固然不錯，但本身用途不大。同樣，僅僅在 Colab notebook 中運行這些模型也意義不大：如果能在本機上運行會更方便。請繼續閱讀下一節，了解如何下載新訓練的模型並在本地設備上運行。

#7.&nbsp;Deploy Model

Now that your custom model has been trained, it's ready to be downloaded and deployed in an application! YOLO models can run on a wide variety of hardware, including PCs, embedded systems, and phones. Ultralytics makes it easy to convert the YOLO models to various formats (`tflite`, `onnx`, etc.) and deploy them in a variety of environments.

This section shows how to download the model and provides links to instructions for deploying it on your PC and edge devices like the Raspberry Pi.

## 7.1 下載 YOLO 模型

首先，執行以下程式碼區塊，將訓練好的模型壓縮並下載。

程式碼會建立一個名為 `my_model` 的資料夾，將模型權重移入其中，並將權重檔案從 `best.pt` 重新命名為 `my_model.pt`。它還會添加訓練結果，以便您稍後引用。然後，它會將該資料夾壓縮成一個名為 `my_model.zip` 的檔案。
"""

# Commented out IPython magic to ensure Python compatibility.
# Create "my_model" folder to store model weights and train results
!mkdir /content/my_model
!cp /content/runs/detect/train/weights/best.pt /content/my_model/my_model.pt
!cp -r /content/runs/detect/train /content/my_model

# Zip into "my_model.zip"
# %cd my_model
!zip /content/my_model.zip my_model.pt
!zip -r /content/my_model.zip train
# %cd /content

# This takes forever for some reason, you can also just download the model from the sidebar
from google.colab import files

files.download('/content/my_model.zip')

"""## 7.2 在本機裝置上部署 YOLO 模型

接下來，我們將下載模型並在本地設備上運行。本節提供在各種裝置上部署 YOLO 模型的說明。

我編寫了一個簡單的 Python 腳本 `yolo_detect.py`，演示瞭如何加載模型、對圖像源進行推理、解析推理結果以及在圖像中每個檢測到的類別周圍顯示方框。這個腳本（https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/blob/main/yolo_detect.py）提供了一個使用 Python 處理 Ultralytics YOLO 模型的範例，可以作為更高級應用程式的起點。

### 7.2.1 在 PC（Windows、Linux 或 macOS）上部署

在 PC 上運行 Ultralytics 模型最簡單的方法是使用 Anaconda。 Anaconda 會創造一個虛擬的 Python 環境，方便您輕鬆安裝 Ultralytics 和 PyTorch。它還會自動安裝 CUDA 和 cuDNN，從而利用 NVIDIA GPU 加速模型推理。

> **注意：** 我的 YouTube 影片（連結待新增）示範如何在 PC 上部署模型。影片示範了以下步驟，如果您更喜歡觀看影片教程，請觀看此影片。

**1. 下載並安裝 Anaconda**

造訪 Anaconda 下載頁面 https://anaconda.com/download，點擊「跳過註冊」按鈕，然後下載適用於您作業系統的軟體包。下載完成後，請執行安裝程式並按照安裝步驟進行操作。您可以使用預設的安裝選項。

**2.設定虛擬環境**

安裝完成後，從開始功能表執行 Anaconda Prompt。 （如果您使用的是 macOS 或 Linux 系統，只需開啟命令終端即可。）

運行以下命令創建新的 Python 環境並啟動它：

```

conda create --name yolo-env1 python=3.12 -y

conda activate yolo-env1

```

執行以下命令安裝 Ultralytics（它也會安裝 OpenCV-Python、Numpy 和 PyTorch 等導入庫）：

```

pip install ultralytics

```

如果您有 NVIDIA GPU，則可以透過執行下列命令安裝支援 GPU 的 PyTorch 版本：

```

pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

```

**3.解壓縮下載的模型**

將您在步驟 7.1 中下載的 `my_model.zip` 檔案解壓縮到您電腦上的一個資料夾。在 Anaconda Prompt 終端機中，使用以下命令進入解壓縮後的資料夾：

```

cd path/to/folder

```

**4. 下載並執行 yolo_detect.py**

使用以下命令將 `yolo_detect.py` 腳本下載到 `my_model` 資料夾中：

```

curl -o yolo_detect.py https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/yolo_detect.py

```

好了！現在可以運行腳本了。若要在解析度為 1280x720 的 USB 攝影機上執行 YOLOv8S 模型推理，請執行下列命令：

```

python yolo_detect.py --model my_model.pt --source usb0 --resolution 1280x720

```

此時將出現一個窗口，顯示來自網路攝影機的即時畫面，並在每個畫面中以方框標記偵測到的物件。

您也可以在視訊檔案、圖像或圖像資料夾上運行該模型。要查看 `yolo_detect.py` 的完整參數列表，請執行 `python yolo_detect.py --help` 或參閱 [README 文件](https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/blob/main/README.)。

### 7.2.2 Deploy on Raspberry Pi

Keep an eye out for an article showing how to convert YOLO models to NCNN format and run them on the Raspberry Pi!

# 8.&nbsp;Conclusion

恭喜！您已成功訓練並部署了 YOLO 目標偵測模型。 😀

接下來，您可以將應用程式的功能擴展到繪製方框和計數目標之外。新增諸如記錄一段時間內偵測到的目標數量或在偵測到特定目標時拍照等功能。請查看我們 GitHub 程式碼庫中的一些範例應用程式：https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models

感謝您完成本筆記本，祝福您的專案一切順利！

# Appendix: Common Errors

如果您在使用本筆記本時遇到任何錯誤，請執行以下操作：

- 仔細檢查資料集檔案是否已正確放置在資料夾結構中

- 確保您的標籤映射檔案中沒有拼字錯誤或其他錯誤

- 使用 Google 搜尋錯誤訊息以尋找解決方案

如果以上方法都無效，請在 GitHub 頁面上提交一個問題（[Issue](https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/issues)）。在本節中，我會陸續添加常見錯誤的解決方案。
"""